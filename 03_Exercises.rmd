---
title: "03 Exercises"
output: html_document
---

# Conceptual

1. Describe the null hypotheses to which the p-values given in **Table 3.4** correspond. Explain what conclusions you can draw based on these p-values. Your explanation should be phrased in terms of sales, TV, radio, and newspaper, rather than in terms of the coefficients of the linear model.

    |  | Coefficient | Std. error | t-statistic | p-value |
    |:--|--:|--:|--:|--:|
    | Intercept | 2.939 | 0.3119 | 9.42 | < 0.0001|
    | TV | 0.046 | 0.0014 | 32.81 | < 0.0001 |
    | radio | 0.189 | 0.0086 | 21.89 | < 0.0001|
    | newspaper | −0.001 | 0.0059 | −0.18 | 0.8599 |
    
    ```
    The null hypothesis here is that product sales do not depend on the advertising budgets for TV, radio or
    newspaper.  In other words,
    
    1. The TV advertising budget does not impact product sales.
    2. The radio advertising budget does not impact product sales.
    3. The newspaper advertising budget does not impact product sales.
    
    Looking at the p-values in the table, this is not completely true.  As the p-values for TV and radio 
    advertising are highly significant, we reject the null hypotheses that they don't have an impact on sales.
    However, we accept the null hypothesis for newspaper advertising as the p-value here is not significant.
    
    1. If the TV advertising budget was increased by $1000 while keeping the radio and newspaper budgets
       unchanged, an additional 46 product units will be sold.
    2. If the radio budget was increased by $1000 while keeping the TV and newspaper budgets unchanged, an 
       additional 189 units will be sold.
    3. Finally, if the newspaper budget was increased by $1000 while keeping the TV and radio budgets unchanged,
       product sales will decrease by 1 unit.
    ```
    
---

2. Carefully explain the differences between the **KNN classifier** and **KNN regression** methods.


    ```
    KNN uses a non-parametric way to predict the output for a given observation.  It uses the 
    nearest K neighbors to the target to determine the result.  While the KNN classifier provides a 
    qualitative output, KNN regression delivers a quantitative response.
    
    In the case of the KNN classifier, the output is the class to which a majority of the K Nearest Neighbors 
    to the target belong.
    
    In the case of the KNN regression method, the result is the average of the output linked to the K Nearest 
    Neighbors of the target.
    
    ```

---
    
3. Suppose we have a data set with five predictors, $x_1$ = GPA, $x_2$ = IQ, $x_3$ = Gender (1 for Female and 0 for Male), $x_4$ = Interaction between GPA and IQ, and $x_5$ = Interaction between GPA and Gender. The response is starting salary after graduation (in thousands of dollars). Suppose we use least squares to fit the model, and get $\hat\beta_0$ = 50, $\hat\beta_1$ = 20, $\hat\beta_2$ = 0.07, $\hat\beta_3$ = 35, $\hat\beta_4$ = 0.01, $\hat\beta_5$ = −10.

    (a) Which answer is correct, and why?
    
        i. For a fixed value of IQ and GPA, males earn more on average than females.
        ii. For a fixed value of IQ and GPA, females earn more on average than males.
        iii. For a fixed value of IQ and GPA, males earn more on average than females provided that the GPA is high enough.
        iv. For a fixed value of IQ and GPA, females earn more on average than males provided that the GPA is high enough.
        
            ```
            Salary = 50 + 20 * GPA + 0.07 * IQ + 35 * 1 + 0.01 * GPA:IQ - 10 * GPA * 1 (for Females)
                   = 85 + 10 * GPA + 0.07 * IQ + 0.01 * GPA:IQ
                   
            Salary = 50 + 20 * GPA + 0.07 * IQ + 35 * 0 + 0.01 * GPA:IQ - 10 * GPA * 0 (for Males)
                   = 50 + 20 * GPA + 0.07 * IQ + 0.01 * GPA:IQ
                   
            iii. Based on the formulas shown above, for a fixed value of IQ and GPA, males earn more on 
                 average than females provided that the GPA is high enough.

            ```

    (b) Predict the salary of a female with IQ of 110 and a GPA of 4.0.
    
        ```
        Salary = 85 + 10 * 4 + 0.07 * 110 + 0.01 * 4.0 * 110
               = 137.1 (or $137,100)
               
        ```
    
    (c) True or false: Since the coefficient for the GPA/IQ interaction term is very small, there is very little evidence of an interaction effect. Justify your answer.
    
        ```
        False. The p-value of the regression coefficient needs to be examined to determine if the interaction
        term is statstically significant or not.
        
        ```
        
---

4. I collect a set of data (n = 100 observations) containing a single predictor and a quantitative response. I then fit a linear regression model to the data, as well as a separate cubic regression, i.e. $Y = β_0 + β_1X + β_2X^2 + β_3X^3 + \varepsilon$.

    (a) Suppose that the true relationship between X and Y is linear, i.e. $Y = β_0 + β_1X + \varepsilon$. Consider the training residual sum of squares (RSS) for the linear regression, and also the training RSS for the cubic regression. Would we expect one to be lower than the other, would we expect them to be the same, or is there not enough information to tell? Justify your answer.
    
        ```
        Polynomial regression models are much more flexible compared to linear regression. Hence, the cubic 
        regression model is likely to accommodate for the irreducible error present in the training data.
        This should make it a tigther fit when compared to the linear regression model. Based on that, the  
        training RSS for the cubic model is expected to be lower than that of the linear model.
        ```
        
    (b) Answer (a) using test rather than training RSS.
    
        ```
        The test RSS for the linear model is expected to be lower.
        ```
    
    (c) Suppose that the true relationship between X and Y is not linear, but we don’t know how far it is from linear. Consider the training RSS for the linear regression, and also the training RSS for the cubic regression. Would we expect one to be lower than the other, would we expect them to be the same, or is there not enough information to tell? Justify your answer.

        ```
        Polynomial regression models are much more flexible compared to linear regression. Hence, the cubic 
        regression model is likely to accommodate for the irreducible error present in the training data.
        This should make it a tigther fit when compared to the linear regression model. Based on that, the  
        training RSS for the cubic model is expected to be lower than that of the linear model.
        ```

    (d) Answer (c) using test rather than training RSS.
    
        ```
        Not knowing if the true relationship is closer to being linear or cubic in nature, there is not
        enough information to determine if the test RSS of one model is lower than the other.
        ```

---

5. Consider the fitted values that result from performing linear regression without an intercept. In this setting, the $i^{th}$ fitted value takes the form 

    $\hat y_i = x_i\hat β$, where $\hat \beta = \frac {\sum_{i=1}^n x_iy_i} {\sum_{i'=1}^n x_{i'}^2}$.
    Show that we can write $\hat y_i = \sum_{i'=1}^n a_{i'}y_{i'}$.
    What is  $a_{i'}$?

    *Note: We interpret this result by saying that the fitted values from linear regression are linear combinations of the response values.*

    **Answer**:
    
    $\hat y_i = x_i\hat β 
    = x_i \frac {\sum_{j=1}^n x_jy_j} {\sum_{i'=1}^n x_{i'}^2} 
    = \frac {\sum_{j=1}^n x_ix_jy_j} {\sum_{i'=1}^n x_{i'}^2} 
    = \sum_{j=1}^n (\frac {x_ix_jy_j} {\sum_{i'=1}^n x_{i'}^2})
    = \sum_{j=1}^n \frac {x_ix_j} {\sum_{i'=1}^n x_{i'}^2} y_j$ 
    
    If $a_j = \frac {x_i x_j} {\sum_{i'=1}^n x_{i'}^2}$, then we have  $\hat y_i = \sum_{j=1}^n a_j y_j$ 
    
    In other words, $\hat y_i = \sum_{i'=1}^n a_{i'} y_{i'}$

---

6. Using **(3.4)**, argue that in the case of simple linear regression, the least squares line always passes through the point $(\bar x, \bar y)$.

    **Answer:**
    
    We have $\hat y = \hat \beta_0 + \hat \beta_1 \hat x$
    
    where $\hat \beta_1 = \frac {\sum_{i=1}^n (x_i - \bar x)(y_i - \bar y)} {\sum_{i=1}^n (x_i - \bar x)^2}$
    and $\hat \beta_0 = \bar y - \hat \beta_1 \bar x$.
    
    Substituting for $\hat \beta_0$, we have
    
    $\hat y = \bar y - \hat \beta_1 \bar x + \hat \beta_1 \hat x$
    
    If $\hat x = \bar x$, then 
    
    $\hat y = \bar y - \hat \beta_1 \bar x + \hat \beta_1 \bar x = \bar y$
    
    Hence, the least squares line always passes through the point $(\bar x, \bar y)$

---

7. It is claimed in the text that in the case of simple linear regression of Y onto X, the $R^2$ statistic **(3.17)** is equal to the square of the correlation between X and Y **(3.18)**. Prove that this is the case. For simplicity, you may assume that $\bar x = \bar y = 0$.

    **Answer**:
    
    **Objective**: Prove that in the case of simple linear regression 
    $y = \beta_0 + \beta_1 x + \varepsilon$, 
    $R^2$ is equal to the square of correlation between x and y.
    In other words, $R^2 = [corr(x, y)]^2$

    We know the following...

    $R^2 = 1 - \frac{RSS}{TSS}$
    where $RSS = \sum (y_i - \hat{y}_i)^2$ 
    and $TSS = \sum (y_i - \bar{y})^2$

    Also, $corr(x, y) = \frac{\sum (x_i - \bar{x}) (y_i - \bar{y})} {\sigma_x \sigma_y}$
    where $\sigma_x = \sqrt{\sum (x_i - \bar{x})^2}$
    and $\sigma_y = \sqrt{\sum (y_i - \bar{y})^2}$
    
    **Proof**:

    $R^2 = 1 - \frac{RSS}{TSS}$
    $= \frac{TSS - RSS}{TSS}$
    $= \frac {\sum (y_i - \bar{y})^2 - \sum (y_i - \hat{y}_i)^2} {\sum (y_i - \bar{y})^2}$

    Focusing on just the numerator:

    Numerator = $\sum (y_i - \bar{y})^2 - \sum (y_i - \hat{y}_i)^2$
    $= \sum [(y_i - \bar{y})^2 - (y_i - \hat{y}_i)^2]$
    $= \sum [ (y_i - \bar{y}) - (y_i - \hat{y}_i) ] [ (y_i - \bar{y}) + (y_i - \hat{y}_i)]$
    
    $\implies$ Numerator = $\sum ( \hat{y}_i - \bar{y} ) ( 2y_i - \bar{y} - \hat{y}_i )$
    
    We know that:
    $\hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x}$ and 
    $\hat{\beta}_1 = \frac{\sum (x_i - \bar{x})(y_i - \bar{y})} {\sum (x_j - \bar{x})^2}$


    Substituting the expression for $\hat{\beta}_0$ into $\hat{y}_i$, we have:
    
    $\hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1 x_i$
    $= \bar{y} - \hat{\beta}_1 \bar{x} + \hat{\beta}_1 x_i$
    $= \bar{y} + \hat{\beta}_1 (x_i - \bar{x})$ 
    
    In other words,
    $\hat{y}_i - \bar{y} = \hat{\beta}_1 (x_i - \bar{x})$
    
    This accounts for the first term in the numerator.  Now, looking at the second term there, we have:
    
    $( 2y_i - \bar{y} - \hat{y}_i ) = 2y_i - \bar{y} - \bar{y} - \hat{\beta}_1 (x_i - \bar{x})$
    $= 2(y_i - \bar{y}) - \hat{\beta}_1 (x_i - \bar{x})$
    
    We now have the Numerator as follows:
    
    Numerator = $\sum [\hat{\beta}_1 (x_i - \bar{x})] [2(y_i - \bar{y}) - \hat{\beta}_1 (x_i - \bar{x})]$ 

    $\implies$ Numerator = $\hat{\beta}_1 \sum (x_i - \bar{x}) [2(y_i - \bar{y}) - \hat{\beta}_1 (x_i - \bar{x})]$ 
  
    $\implies$ Numerator = $\hat{\beta}_1 \sum [2(x_i - \bar{x})(y_i - \bar{y}) - \hat{\beta}_1 (x_i - \bar{x})^2]$ 

    $\implies$ Numerator = $\hat{\beta}_1 [ 2 \sum (x_i - \bar{x})(y_i - \bar{y}) - \hat{\beta}_1 \sum (x_i - \bar{x})^2]$ 

    Substituting the value for $\hat{\beta}_1$ in the last term, we get:
    
    Numerator = $\hat{\beta}_1 [ 2 \sum (x_i - \bar{x})(y_i - \bar{y}) - \sum (x_i - \bar{x})(y_i - \bar{y})]$
    
    $\implies$ Numerator = $\hat{\beta}_1 [ \sum (x_i - \bar{x})(y_i - \bar{y}) ]$
    
    Substituting for $\hat{\beta}_1$ again, we have:
    
    Numerator = $\frac {[ \sum (x_i - \bar{x})(y_i - \bar{y}) ]^2}{\sum (x_i - \bar{x})^2}$
    
    So, we end up with 
    
    $R^2 = \frac {[ \sum (x_i - \bar{x})(y_i - \bar{y}) ]^2}{\sum (x_i - \bar{x})^2 \sum (y_i - \bar{y})^2}$
    $= [\frac {[ \sum (x_i - \bar{x})(y_i - \bar{y}) ]}{\sqrt{\sum (x_i - \bar{x})^2} \sqrt{\sum (y_i - \bar{y})^2}}]^2$
    $= [corr(x, y)]^2$

---
